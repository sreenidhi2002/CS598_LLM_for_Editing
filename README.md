# CS598KD Project - Multimodal LLM for Subject-Driven Image Editing

Our project tests out subject-driven imporvments in image editing though the use of multimodal large language models (LLMs) to see if it can perform better on maintaining subject identity and spatial accuracy in the edited outputs.

### Key Challenges
- Ensuring that the embeddings generated by LLaMA-VID are compatible with the encoder in the BLIP-Diffusion model.
- Evaluating how well the model preserves subject identity and manages spatial reasoning.

## Project Setup Instructions

#### 1. Clone the Repository to your local machine:

```bash
git clone https://github.com/sreenidhi2002/CS598_LLM_for_Editing.git
cd CS598_LLM_for_Editing
```

#### 2. Setup instructions for each model:

- LLaMA-VID: The code for LLaMA-VID is available in the llama_vid directory. In order to access the directory for LLAVA-VID, use the following code:
```
cd LLaMA-VID
```

- BLIP-Diffusion: The BLIP-Diffusion code is located in the blip_diffusion directory. The additional setup steps are listed within this directory to ensure the model can run correctly.
```
cd LAVIS
source setup_env_lavis.sh
```

Use the DataLoaders to input images from both datasets and test the modelâ€™s performance on subject-driven editing tasks. The dataloaders can be found under the "Dataloaders" directory and can be accessed as follows:
```
cd LAVIS
```

#### 3. Testing: 
The code for the evaluation can be accessed through the Evaluation folder as shown here:
```
cd Evaluation
python clip_score_evaluation.py
```

### Contributors
- Mingjun Liu
- Yen-Chi Cheng
- Sreenidhi Vijayaraghavan
